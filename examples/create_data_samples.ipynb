{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_dict = {\n",
    "    'ViNLI': '../data/vinli/UIT_ViNLI_1.0_{split}.jsonl',\n",
    "    'SNLI': 'data/snli/snli_1.0_{split}.jsonl',\n",
    "    'MultiNLI': 'data/multinli/multinli_1.0_{split}.jsonl',\n",
    "    'Contract_NLI':'data/contract-nli/{split}.json'\n",
    "}   \n",
    "split_dict = ['train', 'test', 'dev']\n",
    "tokenizer_dict = {\n",
    "    'xlmr': \"xlm-roberta-large\",\n",
    "    't5': \"t5-large\",\n",
    "    'phobert': \"vinai/phobert-large\",\n",
    "}\n",
    "# label_dict = {\n",
    "#     \"contradiction\": 0,\n",
    "#     \"neutral\": 1,\n",
    "#     \"entailment\": 2,\n",
    "#     \"other\": 3,\n",
    "#     \"-\": -1\n",
    "# }\n",
    "contract_label_dict = {\n",
    "    \"Contradiction\": 0,\n",
    "    \"NotMentioned\": 1,\n",
    "    \"Entailment\":2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_max_length(dataset, split_dict, tokenize_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dict[tokenize_name])\n",
    "    dataset = dataset.map(lambda examples: tokenizer(\n",
    "        examples[\"sentence2\"], \n",
    "        examples[\"sentence1\"],\n",
    "        ), batched=True)\n",
    "    # Mergedata = concatenate_datasets([dataset[split_dict[0]],dataset[split_dict[1]],dataset[split_dict[2]]])\n",
    "    # sorted_sequences = sorted(enumerate(Mergedata['attention_mask']), key=lambda x: len(x[1]), reverse=True)\n",
    "    sorted_sequences = sorted(enumerate(dataset['train']['attention_mask']), key=lambda x: len(x[1]), reverse=True)\n",
    "    sorted_indices, sorted_sequences = zip(*sorted_sequences)\n",
    "    return len(sorted_sequences[0])\n",
    "\n",
    "def TakeSampleDataset(dataset, split_dict, num_sample):\n",
    "    dataset[split_dict[0]] = dataset[split_dict[0]].select(range(num_sample))\n",
    "    dataset[split_dict[1]] = dataset[split_dict[0]].select(range(num_sample))\n",
    "    dataset[split_dict[2]] = dataset[split_dict[0]].select(range(num_sample))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f79f35a4c91425a9d037e8c48a45d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24376 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39eb897764294edda223bb8f82c3a39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2991 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d8bc65ef314f0f967e491222f8963a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize_name = \"xlmr\"\n",
    "data_files = {}\n",
    "for split in split_dict:\n",
    "    path = dataset_path_dict['ViNLI'].format(split=split)\n",
    "    data_files[split] = path\n",
    "dataset = load_dataset(\"json\", data_files=data_files).filter(lambda example: example['gold_label'] != '-')\n",
    "# if not self.load_all_labels:\n",
    "#     dataset = dataset.filter(lambda example: example['gold_label'] != 'other')\n",
    "# dataset = dataset.map(lambda example: {\"labels\": label_dict[example[\"gold_label\"]]}, remove_columns=[\"gold_label\"])\n",
    "max_length = Find_max_length(dataset, split_dict, tokenize_name)\n",
    "data_path = f'data_tokenized/{tokenize_name}/vinli/{max_length}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pairID', 'gold_label', 'link', 'context', 'sentence1', 'sentenceID', 'topic', 'sentence2', 'annotator_labels'],\n",
       "    num_rows: 24376\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['pairID', 'link', 'context', 'sentenceID', 'topic', 'annotator_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['gold_label', 'sentence1', 'sentence2'],\n",
       "    num_rows: 24376\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entailment',\n",
       " 'entailment',\n",
       " 'contradiction',\n",
       " 'contradiction',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'other',\n",
       " 'other',\n",
       " 'entailment',\n",
       " 'entailment']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['gold_label'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_entailment(examples):\n",
    "    if examples['gold_label'] != 'entailment':\n",
    "        examples['gold_label'] = 'non_entailment'\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be11c1c57fa14e8f8f3fde8a72e907f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24376 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(map_entailment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['gold_label', 'sentence1', 'sentence2'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee517b70e71343718a56357927e41b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1d0ebe64344d0ca0e58f7043434a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"vinli_entailment_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
