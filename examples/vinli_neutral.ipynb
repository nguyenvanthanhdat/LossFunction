{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset, load_from_disk\n",
    "import pandas\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"presencesw/vinli_3_label\"\n",
    "label = 'neutral'\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['gold_label', 'sentence1', 'sentence2'],\n",
      "        num_rows: 18282\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['gold_label', 'sentence1', 'sentence2'],\n",
      "        num_rows: 2264\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['gold_label', 'sentence1', 'sentence2'],\n",
      "        num_rows: 2255\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_entailment(exmaples):\n",
    "    labels = exmaples['gold_label']\n",
    "    labels = [f\"not_{label}\" if i != label else label for i in labels]\n",
    "    exmaples['gold_label'] = labels\n",
    "    return exmaples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['gold_label', 'sentence1', 'sentence2'],\n",
       "    num_rows: 18282\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this step convert dataset to 2 labels\n",
    "converted_dataset = dataset.map(convert_entailment, batched=True)\n",
    "\n",
    "# check number samples in dataset train\n",
    "converted_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divine the dataset to 2 sets `entailment` and `not entailment`\n",
    "new_dataset_entail = converted_dataset.filter(lambda example: example['gold_label'] == label)['train']\n",
    "new_dataset_not_entail = converted_dataset.filter(lambda example: example['gold_label'] == f'not_{label}')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataset to pandas for next step\n",
    "new_dataset_entail = Dataset.to_pandas(new_dataset_entail)\n",
    "# duplicate dataset not entailment\n",
    "new_dataset_not_entail1 = Dataset.to_pandas(new_dataset_not_entail)\n",
    "new_dataset_not_entail2 = Dataset.to_pandas(new_dataset_not_entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12188/12188 [00:01<00:00, 6416.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# merge 2 dataset not entailment for the target\n",
    "j = 0\n",
    "for i in tqdm(range(len(new_dataset_not_entail1))):\n",
    "    temp = set()\n",
    "    # check = False\n",
    "    # for j in range(len(new_dataset_not_entail2)):\n",
    "    for index in range(100):\n",
    "        new_j = j + index\n",
    "        if new_j == len(new_dataset_not_entail1):\n",
    "            j = new_j\n",
    "            break\n",
    "        if new_dataset_not_entail1.iloc[i]['sentence1'] != new_dataset_not_entail2.iloc[new_j]['sentence1']:\n",
    "            j = new_j\n",
    "            break\n",
    "        temp.add(new_dataset_not_entail2.iloc[new_j]['sentence2'])\n",
    "    # new_dataset_not_entail1.iloc[i]['sentence2'] = temp\n",
    "    # temp = [val for val in temp]\n",
    "    new_dataset_not_entail1.at[i, 'sentence2'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3044\n"
     ]
    }
   ],
   "source": [
    "# dataset_not_entail = new_dataset_not_entail1.drop_duplicates(subset=['sentence2'])\n",
    "# print(len(dataset_not_entail))\n",
    "dataset_not_entail = new_dataset_not_entail1[new_dataset_not_entail1['sentence2'] != set()]\n",
    "print(len(dataset_not_entail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not_neutral</td>\n",
       "      <td>Sau dự án điện mặt trời, Tập đoàn Trung Nam vừ...</td>\n",
       "      <td>{Tập đoàn Trung Nguyên vừa bán hơn hơn 30% cổ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not_neutral</td>\n",
       "      <td>Phía Tập đoàn Trung Nam cho biết, với 64,9% cổ...</td>\n",
       "      <td>{Tập đoàn Trung Nam nắm giữ phần lớn lượng cổ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not_neutral</td>\n",
       "      <td>Cách đây một tháng, tập đoàn này cũng bán 49% ...</td>\n",
       "      <td>{Vốn của một công ty nước ngoài đang chiếm 49%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not_neutral</td>\n",
       "      <td>Dự án Nhà máy Điện gió Trung Nam, tổng số vốn ...</td>\n",
       "      <td>{Sản lượng điện dự kiến tạo ra trong một năm t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not_neutral</td>\n",
       "      <td>Từ nay tới năm 2028, dòng vốn đầu tư được dự b...</td>\n",
       "      <td>{Nguyên nhân cho việc dòng vốn đầu tư từ nay c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>not_neutral</td>\n",
       "      <td>Ổ SSD dùng để khai thác tiền số Chia chỉ còn t...</td>\n",
       "      <td>{Dùng ổ cứng SSD để khai thác tiền số Chia làm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>not_neutral</td>\n",
       "      <td>Khi khai thác Chia, những ổ cứng dung lượng lớ...</td>\n",
       "      <td>{Các ổ cứng 512 GB sẽ có tuổi thọ cao hơn so v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3041</th>\n",
       "      <td>not_neutral</td>\n",
       "      <td>Ổ cứng thường được hãng bảo hành từ ba đến năm...</td>\n",
       "      <td>{Người dùng chỉ được bảo hành trong 1 năm sau ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>not_neutral</td>\n",
       "      <td>Ổ SSD được \"thợ đào\" Chia yêu thích vì tốc độ ...</td>\n",
       "      <td>{Nhiều người dùng ổ cứng SSD để khai thác tiền...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>not_neutral</td>\n",
       "      <td>Chia là tiền điện tử đang \"sốt\" tại Trung Quốc.</td>\n",
       "      <td>{Rất ít người ở Trung Quốc biết đến sự tồn tại...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3044 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gold_label                                          sentence1  \\\n",
       "0     not_neutral  Sau dự án điện mặt trời, Tập đoàn Trung Nam vừ...   \n",
       "1     not_neutral  Phía Tập đoàn Trung Nam cho biết, với 64,9% cổ...   \n",
       "2     not_neutral  Cách đây một tháng, tập đoàn này cũng bán 49% ...   \n",
       "3     not_neutral  Dự án Nhà máy Điện gió Trung Nam, tổng số vốn ...   \n",
       "4     not_neutral  Từ nay tới năm 2028, dòng vốn đầu tư được dự b...   \n",
       "...           ...                                                ...   \n",
       "3039  not_neutral  Ổ SSD dùng để khai thác tiền số Chia chỉ còn t...   \n",
       "3040  not_neutral  Khi khai thác Chia, những ổ cứng dung lượng lớ...   \n",
       "3041  not_neutral  Ổ cứng thường được hãng bảo hành từ ba đến năm...   \n",
       "3042  not_neutral  Ổ SSD được \"thợ đào\" Chia yêu thích vì tốc độ ...   \n",
       "3043  not_neutral    Chia là tiền điện tử đang \"sốt\" tại Trung Quốc.   \n",
       "\n",
       "                                              sentence2  \n",
       "0     {Tập đoàn Trung Nguyên vừa bán hơn hơn 30% cổ ...  \n",
       "1     {Tập đoàn Trung Nam nắm giữ phần lớn lượng cổ ...  \n",
       "2     {Vốn của một công ty nước ngoài đang chiếm 49%...  \n",
       "3     {Sản lượng điện dự kiến tạo ra trong một năm t...  \n",
       "4     {Nguyên nhân cho việc dòng vốn đầu tư từ nay c...  \n",
       "...                                                 ...  \n",
       "3039  {Dùng ổ cứng SSD để khai thác tiền số Chia làm...  \n",
       "3040  {Các ổ cứng 512 GB sẽ có tuổi thọ cao hơn so v...  \n",
       "3041  {Người dùng chỉ được bảo hành trong 1 năm sau ...  \n",
       "3042  {Nhiều người dùng ổ cứng SSD để khai thác tiền...  \n",
       "3043  {Rất ít người ở Trung Quốc biết đến sự tồn tại...  \n",
       "\n",
       "[3044 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_not_entail = dataset_not_entail.reset_index(drop=True)\n",
    "dataset_not_entail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6094 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6094/6094 [04:28<00:00, 22.73it/s]\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for index1 in tqdm(range(len(new_dataset_entail))):\n",
    "    # print(new_dataset_entail.iloc[index1]['sentence1'])\n",
    "    for index2 in range(len(dataset_not_entail)):\n",
    "        if new_dataset_entail.iloc[index1]['sentence1'] == dataset_not_entail.iloc[index2]['sentence1']:\n",
    "            temp.append(dataset_not_entail.iloc[index2]['sentence2'])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6094\n"
     ]
    }
   ],
   "source": [
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset_entail['negative'] = temp\n",
    "# new_dataset_entail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['gold_label', 'anchor', 'positive', 'negative'],\n",
       "    num_rows: 6094\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = Dataset.from_pandas(new_dataset_entail)\n",
    "new_dataset = new_dataset.rename_column(\"sentence1\", \"anchor\")\n",
    "new_dataset = new_dataset.rename_column(\"sentence2\", \"positive\")\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5815360423be46c994c28709abfe3a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_dataset(example):\n",
    "    # print(example['negative'])\n",
    "    number = len(example['negative'][0])\n",
    "    # print(gold_label)\n",
    "    gold_label = example['gold_label'] * number\n",
    "    anchor = example['anchor'] * number\n",
    "    positive = example['positive'] * number\n",
    "    return {'gold_label': gold_label, 'anchor': anchor, \n",
    "            'positive': positive, 'negative': example['negative'][0]}\n",
    "\n",
    "        \n",
    "new_dataset = new_dataset.map(\n",
    "    lambda example: convert_dataset(example),                   \n",
    "    remove_columns=['gold_label', 'anchor', 'positive', 'negative'],\n",
    "    batched=True,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['gold_label', 'anchor', 'positive', 'negative'],\n",
       "    num_rows: 24406\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(['']*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_columns(examples):\n",
    "    # print(len(examples))\n",
    "    length = len(examples['gold_label'])\n",
    "    examples['negative'] = [''] * length\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_dataset = converted_dataset.rename_column(\"sentence1\", \"anchor\")\n",
    "converted_dataset = converted_dataset.rename_column(\"sentence2\", \"positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_dataset = converted_dataset.map(add_columns, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = DatasetDict()\n",
    "all_data['train'] = new_dataset\n",
    "all_data['validation'] = converted_dataset['dev']\n",
    "all_data['test'] = converted_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8a887cf0a64ee7bc35b45321feea8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97762d12ae7849c2a88e244d295dcfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb67f89283c46a48c34d69fd7244685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f163b74e9b9b476e8d5c328c33c84f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898370b40c9e43c5837428d470fef837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c0fb90c58c4ec2a681ff60cc89af49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/presencesw/vinli_3_label_neutral/commit/0b546ea05a84bfa5ee851a026be5daeac70b9468', commit_message='Upload dataset', commit_description='', oid='0b546ea05a84bfa5ee851a026be5daeac70b9468', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.push_to_hub(f\"{dataset_name}_{label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
